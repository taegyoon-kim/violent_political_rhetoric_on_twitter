{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "violent_political_rhetoric_classifier_BERT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXiC0F-y_2Bc"
      },
      "source": [
        "Author Note (Taegyoon Kim, taegyoon@psu.edu)\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "- This is a notebook for a BERT fine-tuned classifier introduced in **Taegyoon Kim. Violent Political Rhetoric on Twitter. *Political Science Research and Methods***.\n",
        "- The input data (i.e., training set) is labeled tweets that contain one or more of the violent keywords extracted using the violent keyword extractor (https://github.com/taegyoon-kim/violent_political_rheotric_on_twitter/blob/master/violent_political_rhetoric_violent_keyword_extract.py). \n",
        "- The classifier is trained on GPU provided Google Colaboratory.\n",
        "- Load input data (e.g., from a Google Drive as in the below) and train a classifier. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1UmhiynTVYF"
      },
      "source": [
        "Mount Google Drive\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IchOpHbOWgwo"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNfWpwgX5Yan"
      },
      "source": [
        "Packages\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEQWPt7bg2Ek"
      },
      "source": [
        "!pip install torch torchvision\n",
        "!pip install transformers==2.10.0\n",
        "!pip install seqeval\n",
        "!pip install tensorboardx\n",
        "!pip install simpletransformers==0.9.1 # the classifier is based on simpletransformers package https://github.com/ThilinaRajapakse/simpletransformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_UAgUZkgqJm"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import gc\n",
        "import requests\n",
        "import os\n",
        "\n",
        "from simpletransformers.classification import ClassificationModel\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import f1_score, accuracy_score, recall_score, precision_score, confusion_matrix\n",
        "from scipy.special import softmax\n",
        "\n",
        "import random\n",
        "\n",
        "import torch\n",
        "\n",
        "print(\"Cuda available\" if torch.cuda.is_available() is True else \"CPU\")\n",
        "print(\"PyTorch version: \", torch.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4k5z4eI51Q7"
      },
      "source": [
        "Load Data\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mj5WtN6LnTzI"
      },
      "source": [
        "url = '/content/drive/My Drive/diss_detection/diss_detection_training.csv' # location of training set\n",
        "\n",
        "df = pd.read_csv(url)\n",
        "df['text'] = df['status_final_text']\n",
        "df['threat'] = df['final_binary'].astype(float)\n",
        "df = df[['text','threat']]\n",
        "\n",
        "print(len(df['threat']))\n",
        "print(df['threat'].value_counts(normalize = True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iGMeXa7TmB0"
      },
      "source": [
        "Performance Metrics\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xzlfdywQDGm"
      },
      "source": [
        "def report_results(A, B):\n",
        "    A_name = A.name\n",
        "    B_name = B.name\n",
        "    \n",
        "    df = pd.DataFrame({'A': A,\n",
        "                       'B': B})\n",
        "    df = df.dropna()\n",
        "    A = df['A']\n",
        "    B = df['B']\n",
        "    \n",
        "    prec = precision_score(B, A)\n",
        "    rec = recall_score(B, A)\n",
        "    f1 = f1_score(B, A)\n",
        "    acc = accuracy_score(B, A)\n",
        "\n",
        "    performance = [prec, rec, f1, acc]\n",
        "\n",
        "    return performance"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aN_cXZQ_U0PH"
      },
      "source": [
        "Define Set Seed Function\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18lvcsaMUo95"
      },
      "source": [
        "def set_seed(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kdmb_JZ6TtPg"
      },
      "source": [
        "5-fold Cross Validation\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIvxyLW_0IVY"
      },
      "source": [
        "## hyper-parameters\n",
        "\n",
        "args = {\n",
        "   \n",
        "   'output_dir': 'outputs/',\n",
        "   'cache_dir': 'cache/',\n",
        "\n",
        "   'fp16': False,\n",
        "   'fp16_opt_level': 'O1',\n",
        "   'max_seq_length': 250,\n",
        "   'train_batch_size': 8,\n",
        "   'eval_batch_size': 8,\n",
        "   'gradient_accumulation_steps': 1,\n",
        "   'num_train_epochs': 3,\n",
        "   'weight_decay': 0,\n",
        "   'learning_rate': 3e-5,\n",
        "   'adam_epsilon': 1e-8,\n",
        "   'warmup_ratio': 0.06,\n",
        "   'warmup_steps': 0,\n",
        "   'max_grad_norm': 1.0,\n",
        "\n",
        "   'logging_steps': 50,\n",
        "   'evaluate_during_training': False,\n",
        "   'save_steps': 2000,\n",
        "   'eval_all_checkpoints': True,\n",
        "   'use_tensorboard': True,\n",
        "\n",
        "   'overwrite_output_dir': True,\n",
        "   'reprocess_input_data': True\n",
        "   \n",
        "   }\n",
        "\n",
        "\n",
        "## set seed number\n",
        "\n",
        "set_seed(777)\n",
        "\n",
        "\n",
        "## cross validate\n",
        "\n",
        "kf = KFold(n_splits = 5, random_state = 777, shuffle = True)\n",
        "\n",
        "for train_index, val_index in kf.split(df):\n",
        "  \n",
        "  # splitting dataframe\n",
        "    train_df = df.iloc[train_index]\n",
        "    val_df = df.iloc[val_index]\n",
        "  \n",
        "  # defining Model\n",
        "    model = ClassificationModel('bert', 'bert-base-uncased', args = args)\n",
        "  \n",
        "  # train model\n",
        "    model.train_model(train_df)\n",
        "  \n",
        "  # validate model \n",
        "    predictions, raw_outputs = model.predict(val_df['text'])\n",
        "    probabilities = softmax(raw_outputs, axis=1) \n",
        "  \n",
        "  # apply different thresholds   \n",
        "    val_df['BERT_threat_850'] = np.where(probabilities[:,1] >= 0.85, 1, 0)\n",
        "    val_df['BERT_threat_875'] = np.where(probabilities[:,1] >= 0.875, 1, 0)\n",
        "    val_df['BERT_threat_900'] = np.where(probabilities[:,1] >= 0.9, 1, 0)\n",
        "  \n",
        "  # performance\n",
        "    performance_850 = report_results(val_df['BERT_threat_850'], val_df['threat'])\n",
        "    performance_875 = report_results(val_df['BERT_threat_875'], val_df['threat'])\n",
        "    performance_900 = report_results(val_df['BERT_threat_900'], val_df['threat'])\n",
        "    print(performance_850)\n",
        "    print(performance_875)\n",
        "    print(performance_900)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prediction on New Text\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "gMV94bLwE1aW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## train model\n",
        "\n",
        "model = ClassificationModel('bert', 'bert-base-uncased', args = args)\n",
        "model.train_model(df)\n",
        "\n",
        "\n",
        "## generate predictions\n",
        "\n",
        "predictions, raw_outputs = model.predict('your_new_text'])"
      ],
      "metadata": {
        "id": "QilCkVDIEyYl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}